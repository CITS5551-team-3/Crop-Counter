{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import geopandas as gpd\n",
    "import rasterio.mask\n",
    "import fiona\n",
    "import os\n",
    "import numpy as np\n",
    "import shapefile\n",
    "import rasterstats\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import time\n",
    "import cv2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import cast, Optional\n",
    "\n",
    "# Importing necessary programs from the libraries\n",
    "from rasterio.plot import show\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.crs import CRS\n",
    "from rasterio import features\n",
    "from rasterstats import zonal_stats\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# helper funcs\n",
    "\n",
    "# def display_image(img: np.ndarray, image_name: str, *, dir: str = \"./Test_Images\"):\n",
    "#     \"\"\"expects array input in BGR\"\"\"\n",
    "#     img_vals = (img * 255).astype(np.uint8)\n",
    "#     file_path = os.path.join(dir, f\"{image_name}.jpg\")\n",
    "#     cv2.imwrite(file_path, img_vals)\n",
    "#     display(Image(filename=file_path))\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "def display_image(img: np.ndarray, _: Optional[str] = None):\n",
    "    display(PIL.Image.fromarray(cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_BGR2RGB)))\n",
    "\n",
    "def show_contours(img: np.ndarray, contours):\n",
    "    img_with_contours = img.copy()\n",
    "    for contour in contours:\n",
    "        cv2.drawContours(img_with_contours, [contour], 0, (0, 1, 0), 2)\n",
    "    display_image(img_with_contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"Test_Images/Test_Image_1.JPG\"\n",
    "IMAGE_BITS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_max_value: int\n",
    "red_channel = math.inf\n",
    "green_channel = math.inf\n",
    "blue_channel = math.inf\n",
    "nir_channel = math.inf\n",
    "re_channel = math.inf\n",
    "\n",
    "_, file_extension = os.path.splitext(FILENAME)\n",
    "file_extension = file_extension[1:].lower()\n",
    "if file_extension in {\"tif\", \"tiff\"}:\n",
    "    # red_channel = 3\n",
    "    # green_channel = 2\n",
    "    # blue_channel = 1\n",
    "    # nir_channel = 4\n",
    "    # re_channel = 5\n",
    "\n",
    "    red_channel = 1\n",
    "    green_channel = 2\n",
    "    blue_channel = 3\n",
    "    # channel 4 looks like alpha on the example tiff\n",
    "elif file_extension in {\"jpg\", \"jpeg\", \"jpe\", \"jif\", \"jfif\", \"jfi\"}:\n",
    "    red_channel = 1\n",
    "    green_channel = 2\n",
    "    blue_channel = 3\n",
    "else:\n",
    "    raise Exception(f\"image file extension '{file_extension}' not currently supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clipping image based on a shape file is not necessary for initial tests since the provided images are smaller than a single plot. \n",
    "# This section may be added later once crop data has been collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_raw: Optional[np.ndarray] = None\n",
    "green_raw: Optional[np.ndarray] = None\n",
    "red_raw: Optional[np.ndarray] = None\n",
    "nir_raw: Optional[np.ndarray] = None\n",
    "re_raw: Optional[np.ndarray] = None\n",
    "\n",
    "# importing the image to be analysed\n",
    "with rasterio.open(FILENAME, 'r') as raster2:\n",
    "    raster2 = cast(rasterio.DatasetReader, raster2)\n",
    "    band_count = cast(int, raster2.count)\n",
    "\n",
    "    if (band_count >= blue_channel):\n",
    "        blue_raw = raster2.read(blue_channel)\n",
    "\n",
    "    if (band_count >= green_channel):\n",
    "        green_raw = raster2.read(green_channel)\n",
    "\n",
    "    if (band_count >= red_channel):\n",
    "        red_raw = raster2.read(red_channel)\n",
    "\n",
    "    if (band_count >= nir_channel):\n",
    "        nir_raw = raster2.read(nir_channel)\n",
    "\n",
    "    if (band_count >= re_channel):\n",
    "        re_raw = raster2.read(re_channel)\n",
    "\n",
    "print(band_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting digital numbers (DN) to reflectance\n",
    "# This step is quite important and highly recommended, unless already have been done while image processing\n",
    "# Images converted to reflectance values should range between 0 - 1 or 0 - 100%\n",
    "# The equation for conversion is ((DN/(2^n)) - 1), where n is the bit size of the camera\n",
    "# Digital cameras generally store images as 8 bit or 16 bit\n",
    "# For this example n = 16, and thus ((DN/(2^16)) - 1) = 65535\n",
    "\n",
    "# TODO change to optional\n",
    "blue: np.ndarray\n",
    "green: np.ndarray\n",
    "red: np.ndarray\n",
    "nir: np.ndarray\n",
    "re: np.ndarray\n",
    "\n",
    "image_max_value = 2 ** IMAGE_BITS - 1\n",
    "\n",
    "if blue_raw is not None:\n",
    "    blue = (cast(np.ndarray, blue_raw) / image_max_value).astype(float)\n",
    "\n",
    "if green_raw is not None:\n",
    "    green = (cast(np.ndarray, green_raw) / image_max_value).astype(float)\n",
    "\n",
    "if red_raw is not None:\n",
    "    red = (cast(np.ndarray, red_raw) / image_max_value).astype(float)\n",
    "\n",
    "if nir_raw is not None:\n",
    "    nir = (cast(np.ndarray, nir_raw) / image_max_value).astype(float)\n",
    "\n",
    "if re_raw is not None:\n",
    "    re = (cast(np.ndarray, re_raw) / image_max_value).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vegetation Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This next section involves the calculation of Vegetation indices. \n",
    "# Based on this article https://en.wikipedia.org/wiki/Vegetation_index#:~:text=A%20vegetation%20index%20(VI)%20is,activity%20and%20canopy%20structural%20variations.\n",
    "# A Vegetation Index is a calculation made using multispectral imagery data to highlight different qualities in vegetation such as dead plants or live leaves\n",
    "\n",
    "# Calculating vegetation indices\n",
    "# This example shows with 15 vegetation indices\n",
    "# Any number of vegetation indices can be used\n",
    "\n",
    "# Dealing with the situations division by zero\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "# Making original calculation\n",
    "# Later on adjustments can be made to remove soil, or normalising the vegetation indices, etc.\n",
    "\n",
    "# These arrays give values between 0 and 1 but can also output negative values\n",
    "\n",
    "rgb_indices = False\n",
    "nir_indices = False\n",
    "re_indices = False\n",
    "\n",
    "# RGB only\n",
    "if red_raw is not None and green_raw is not None and blue_raw is not None:\n",
    "    rgb_indices = True\n",
    "\n",
    "    NGRDI_Orig = ((green).astype(float) - (red).astype(float))/((green).astype(float) + (red).astype(float))\n",
    "    HUE = np.arctan((2 * (red - green - blue) )/ (30.5*(green - blue)))\n",
    "\n",
    "    # NIR based\n",
    "    if nir_raw is not None:\n",
    "        nir_indices = True\n",
    "\n",
    "        NDVI_Orig = (nir.astype(float) - red.astype(float)) / (nir.astype(float) + red.astype(float))\n",
    "        GNDVI_Orig = (nir.astype(float) - green.astype(float)) / (nir.astype(float) + green.astype(float))\n",
    "        ENDVI_Orig = (nir.astype(float) + green.astype(float) - 2*blue.astype(float)) / (nir.astype(float) + green.astype(float) + 2*blue.astype(float))\n",
    "        SIPI_Orig = (nir.astype(float)-blue.astype(float))/(nir.astype(float) + red.astype(float))\n",
    "        NLI_Orig = (((nir.astype(float))**2) - red.astype(float)) / (((nir.astype(float))**2) + red.astype(float))\n",
    "        SR_Orig = nir.astype(float)/red.astype(float)\n",
    "        DVI_Orig = nir.astype(float) - red.astype(float)\n",
    "        RDVI_Orig = (nir.astype(float) - red.astype(float)) / ((nir.astype(float) + red.astype(float))**(1/2))\n",
    "\n",
    "        # RE based\n",
    "        if re_raw is not None:\n",
    "            re_indices = True\n",
    "            \n",
    "            RENDVI_Orig = (re.astype(float) - red.astype(float)) / (re.astype(float) + red.astype(float))\n",
    "            NDRE_Orig = (nir.astype(float) - re.astype(float)) / (nir.astype(float) + re.astype(float))\n",
    "            NNIR_Orig = nir.astype(float) / (nir.astype(float) + (re.astype(float) + green.astype(float)))\n",
    "            MCARI_Orig = (re.astype(float)-red.astype(float)) - 2*(re.astype(float) - green.astype(float))*(re.astype(float) / red.astype(float))\n",
    "            MDD_Orig = (nir.astype(float) - re.astype(float)) - (re.astype(float) - green.astype(float))\n",
    "            MARI_Orig = ((1/green.astype(float))-(1/re.astype(float)))*nir.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nir_indices:\n",
    "    # these images scale the 0 - 1 values to a 255 greyscale\n",
    "    # sanity check to make sure the Vegetation Indices are producing images:\n",
    "    display_image(NDVI_Orig, \"VI_Test_NDVI\")\n",
    "    display_image(ENDVI_Orig, \"VI_Test_ENDVI\")\n",
    "    display_image(SIPI_Orig, \"VI_Test_SIPI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop Isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating crop and soil fractions based on NGRDI\n",
    "# This step is optional and based on the need of the research\n",
    "# For this example NGRDI has been used to classify between soil and crop, so anywhere in the image that aligns with NGRDI being negative  will be removed from the selected index\n",
    "# Any vegetation indices can be used (VI_For_Classification)\n",
    "# basic syntax is: \n",
    "# VI = np.where(VI_For_Classification (symbol(s): > or < or = or !=) (classification criteria), VI_Of_Interest, -math.inf)\n",
    "# -math.inf is the number for null values\n",
    "\n",
    "# these arrays contain values ranging from 0 - 1. GNDVI has been tweaked following some experimentation\n",
    "# with the test data found (Test_Image_8) GNDVI seems particularly good at isolating the plants, but with a value of 0 showing the plant and 1 showing shadow\n",
    "# GNDVI also seem to be best in this image for isolating or ignoring shadows. the threshold has also been adjusted based on observation\n",
    "# 0.15 removes a good amount of the soil and less green crops\n",
    "\n",
    "\n",
    "# RGB only\n",
    "if rgb_indices:\n",
    "    NGRDI = np.where(NGRDI_Orig > 0, NGRDI_Orig, -math.inf)\n",
    "\n",
    "# NIR based\n",
    "if nir_indices:\n",
    "    NDVI = np.where(NGRDI_Orig > 0, NDVI_Orig, -math.inf)\n",
    "    GNDVI = np.where(NGRDI_Orig > 0.15, (1-GNDVI_Orig), -math.inf)\n",
    "    ENDVI = np.where(NGRDI_Orig > 0, ENDVI_Orig, -math.inf)\n",
    "    SIPI = np.where(NGRDI_Orig > 0, SIPI_Orig, -math.inf)\n",
    "    NLI = np.where(NGRDI_Orig > 0, NLI_Orig, -math.inf)\n",
    "    SR = np.where(NGRDI_Orig > 0, SR_Orig, -math.inf)\n",
    "    DVI = np.where(NGRDI_Orig > 0, DVI_Orig, -math.inf)\n",
    "    RDVI = np.where(NGRDI_Orig > 0, RDVI_Orig, -math.inf)\n",
    "\n",
    "# RE based\n",
    "if re_indices:\n",
    "    RENDVI = np.where(NGRDI_Orig > 0, RENDVI_Orig, -math.inf)\n",
    "    NDRE = np.where(NGRDI_Orig > 0, NDRE_Orig, -math.inf)\n",
    "    NNIR = np.where(NGRDI_Orig > 0, NNIR_Orig, -math.inf)\n",
    "    MCARI = np.where(NGRDI_Orig > 0, MCARI_Orig, -math.inf)\n",
    "    MDD = np.where(NGRDI_Orig > 0, MDD_Orig, -math.inf)\n",
    "    MARI = np.where(NGRDI_Orig > 0, MARI_Orig, -math.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nir_indices:\n",
    "    # this image scale the 0 - 1 values to a 255 greyscale\n",
    "    # sanity check to make sure the soil exclusion is working, currently no change since NGRDI_Orig already has been classified by itself:\n",
    "    display_image(GNDVI, \"Soil_Exclusion_Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ! TEMP\n",
    "\n",
    "if rgb_indices:\n",
    "    display_image(\n",
    "        np.where(\n",
    "            np.repeat(np.expand_dims(NGRDI_Orig, 2), 3, axis=2) > 0,\n",
    "            cv2.merge([blue, green, red]),\n",
    "            0\n",
    "        ),\n",
    "        \"temp_ndvi\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FieldImageR uses manual plot isolation, equally dividing up a region specified by the user.\n",
    "# For now, a similar approach will be used.\n",
    "\n",
    "# Assumes vertical alignment (or horizontal via transpose)\n",
    "\n",
    "img_rgb = np.squeeze(np.dstack((\n",
    "    cast(np.ndarray, blue),\n",
    "    cast(np.ndarray, green),\n",
    "    cast(np.ndarray, red),\n",
    ")))\n",
    "display_image(img_rgb, \"Img_RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_top_left = (435, 150)\n",
    "plots_bottom_right = (860, 1250)\n",
    "\n",
    "plots_h_range = np.arange(plots_top_left[1], plots_bottom_right[1])\n",
    "plots_v_range = np.arange(plots_top_left[0], plots_bottom_right[0])\n",
    "\n",
    "def plots_range(arr: np.ndarray):\n",
    "    return arr[plots_h_range[:, None], plots_v_range[None, :]]\n",
    "\n",
    "plots_rgb = plots_range(img_rgb)\n",
    "display_image(plots_rgb, \"Plot_RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_rgb_no_soil = np.where(plots_range(np.repeat(np.expand_dims(NGRDI_Orig, 2), 3, axis=2)) > 0.15, plots_rgb, np.zeros(plots_rgb.shape))\n",
    "display_image(plots_rgb_no_soil, \"Plot_RGB_no_soil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
